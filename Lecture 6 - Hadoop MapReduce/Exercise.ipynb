{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyyMBljDqWk8"
      },
      "source": [
        "## Hadoop MapReduce\n",
        "- The purpose is to practice using hadoop software framework by executing map reduce tasks that count words and bigrams in Sherlock Holmes book"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eeq_5gP_vjk"
      },
      "source": [
        "### Install Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_TNFkoZrSKt",
        "outputId": "ef8410cf-25f0-4a88-b178-5adf2d9a061c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-05 15:19:30--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500749234 (478M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.0.tar.gz’\n",
            "\n",
            "hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  18.1MB/s    in 27s     \n",
            "\n",
            "2022-03-05 15:19:58 (17.5 MB/s) - ‘hadoop-3.3.0.tar.gz’ saved [500749234/500749234]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oECBj-r-rYcw"
      },
      "outputs": [],
      "source": [
        "!tar -xzf hadoop-3.3.0.tar.gz\n",
        "!cp -r hadoop-3.3.0/ /usr/local/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCzkbGBkAOc1"
      },
      "source": [
        "### Set up path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K9q6Jv6suzfA"
      },
      "outputs": [],
      "source": [
        "!echo \"export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\" >> \\\n",
        "/usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UPesx3fy0iwG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/hadoop-3.3.0/bin'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBgYMO4qAUuY"
      },
      "source": [
        "### Run Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMMqS3M_2QRb",
        "outputId": "d1281dcf-7774-4ea1-9815-acfbfca5a87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ],
      "source": [
        "!hadoop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ybGPzeAYU3"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yBaj98y-MaI",
        "outputId": "8cec66a9-1d85-4e4d-dbe7-4fd0e3be0e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=130pPxku3vSEOvJb-Wsx1ysF0Fz7WTeKo\n",
            "To: /content/sherlock_text.txt\n",
            "100% 1.13M/1.13M [00:00<00:00, 7.67MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown -O sherlock_text.txt --id 130pPxku3vSEOvJb-Wsx1ysF0Fz7WTeKo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVSJ01jef9UL"
      },
      "source": [
        "### Download scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZODpkuAhf85Q",
        "outputId": "e0fa4da8-3a93-4d3e-abc9-7da7bef76d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'map-reduce'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 44 (delta 15), reused 19 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ppkgtmm/big-data-map-reduce.git map-reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noD4SFDkgPSI",
        "outputId": "5ef5954a-f01f-485e-d536-a474f6c189e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: mapper.py [-h] [--clean] count_type\n",
            "mapper.py: error: the following arguments are required: count_type\n"
          ]
        }
      ],
      "source": [
        "!python3 map-reduce/mapper.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNh_lhbU_oiv"
      },
      "source": [
        "### Steps\n",
        "- Copy file from local machine to HDFS. Show your command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZzbYVci-wO6",
        "outputId": "4467665a-89f5-493c-9e65-4f414765f75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root    1126637 2022-03-05 15:21 /hduser/word_count/input/text.txt\n"
          ]
        }
      ],
      "source": [
        "!hadoop fs -mkdir /hduser\n",
        "!hadoop fs -mkdir /hduser/word_count\n",
        "!hadoop fs -mkdir /hduser/word_count/input\n",
        "!hadoop fs -put ./sherlock_text.txt /hduser/word_count/input/text.txt\n",
        "!hadoop fs -ls /hduser/word_count/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P518kRCd-zPJ",
        "outputId": "04c4c639-b6ec-4f9b-cf0e-0abb3570e68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "A STUDY IN SCARLET.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "PART I.\r\n",
            "\r\n",
            "(_Being a reprint from the reminiscences of_ JOHN H. WATSON, M.D., _late\r\n",
            "of the Army Medical Department._) [2]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I. MR. SHERLOCK HOLMES.\r\n",
            "\r\n",
            "\r\n",
            "IN the year 1878 I took my degree of Doctor of Medicine of the\r\n",
            "University of London, and proceeded to Netley to go through the course\r\n",
            "prescribed for surgeons in the army. Having completed my studies there,\r\n",
            "I was duly attached to the Fifth Northumberland Fusiliers as Assistant\r\n",
            "Surgeon. The regiment was stationed in India at the time, and before\r\n",
            "I could join it, the second Afghan war had broken out. On landing at\r\n",
            "Bombay, I learned that my corps had advanced through the passes, and\r\n",
            "was already deep in the enemy's country. I followed, however, with many\r\n",
            "other officers who were in the same situation as myself, and succeeded\r\n",
            "in reaching Candahar in safety, where I found my regiment, and at once\r\n",
            "entered upon my new duties.\r\n",
            "\r\n",
            "The campaign brought honours and promotion to many, but for me it had\r\n",
            "nothing but "
          ]
        }
      ],
      "source": [
        "!hadoop fs -head /hduser/word_count/input/text.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKWnjxJUAqh6"
      },
      "source": [
        "- Run MapReduce through Hadoop Streaming. Show your result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqqDAisjD77Y",
        "outputId": "3f5cb5dc-8437-4e05-d3c0-6c11dfa1954c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\t1\n",
            "STUDY\t1\n",
            "IN\t1\n",
            "SCARLET.\t1\n",
            "PART\t1\n",
            "I.\t1\n",
            "(_Being\t1\n",
            "a\t1\n",
            "reprint\t1\n",
            "from\t1\n",
            "the\t1\n",
            "reminiscences\t1\n",
            "of_\t1\n",
            "JOHN\t1\n",
            "H.\t1\n",
            "WATSON,\t1\n",
            "M.D.,\t1\n",
            "_late\t1\n"
          ]
        }
      ],
      "source": [
        "!head /content/sherlock_text.txt | python3 /content/map-reduce/mapper.py unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0A7HNOAEWfA",
        "outputId": "d7812331-90eb-4eb5-c2ab-851bd115c99b",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-05 15:21:44,826 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/map-reduce/mapper.py, /content/map-reduce/reducer.py] [] /tmp/streamjob8524567887021395253.jar tmpDir=null\n",
            "2022-03-05 15:21:45,608 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-03-05 15:21:45,851 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-03-05 15:21:45,851 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-03-05 15:21:45,873 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-03-05 15:21:46,085 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-03-05 15:21:46,105 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-03-05 15:21:46,489 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1386624067_0001\n",
            "2022-03-05 15:21:46,489 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-03-05 15:21:46,872 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1386624067_0001_6345fe5e-a4fe-446a-90cb-1610ee3774d6/mapper.py\n",
            "2022-03-05 15:21:46,904 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1386624067_0001_7a2514ad-8294-446b-bb6f-ef5254ef59f9/reducer.py\n",
            "2022-03-05 15:21:47,002 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-03-05 15:21:47,003 INFO mapreduce.Job: Running job: job_local1386624067_0001\n",
            "2022-03-05 15:21:47,009 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-03-05 15:21:47,011 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-03-05 15:21:47,015 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:21:47,015 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:21:47,056 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-03-05 15:21:47,060 INFO mapred.LocalJobRunner: Starting task: attempt_local1386624067_0001_m_000000_0\n",
            "2022-03-05 15:21:47,088 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:21:47,091 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:21:47,117 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-03-05 15:21:47,130 INFO mapred.MapTask: Processing split: file:/hduser/word_count/input/text.txt:0+1126637\n",
            "2022-03-05 15:21:47,158 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-03-05 15:21:47,249 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-03-05 15:21:47,249 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-03-05 15:21:47,249 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-03-05 15:21:47,249 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-03-05 15:21:47,249 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-03-05 15:21:47,254 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-03-05 15:21:47,267 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py, unigram]\n",
            "2022-03-05 15:21:47,280 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-03-05 15:21:47,280 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-03-05 15:21:47,281 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-03-05 15:21:47,281 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-03-05 15:21:47,282 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-03-05 15:21:47,282 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-03-05 15:21:47,283 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-03-05 15:21:47,283 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-03-05 15:21:47,283 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-03-05 15:21:47,286 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-03-05 15:21:47,286 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-03-05 15:21:47,287 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-03-05 15:21:47,320 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:47,320 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:47,322 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:47,343 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:47,450 INFO streaming.PipeMapRed: Records R/W=2682/1\n",
            "2022-03-05 15:21:47,608 INFO streaming.PipeMapRed: R/W/S=10000/43715/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:47,871 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-03-05 15:21:47,872 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-03-05 15:21:47,874 INFO mapred.LocalJobRunner: \n",
            "2022-03-05 15:21:47,878 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-03-05 15:21:47,878 INFO mapred.MapTask: Spilling map output\n",
            "2022-03-05 15:21:47,878 INFO mapred.MapTask: bufstart = 0; bufend = 1502156; bufvoid = 104857600\n",
            "2022-03-05 15:21:47,878 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25402208(101608832); length = 812189/6553600\n",
            "2022-03-05 15:21:48,008 INFO mapreduce.Job: Job job_local1386624067_0001 running in uber mode : false\n",
            "2022-03-05 15:21:48,009 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-03-05 15:21:48,470 INFO mapred.MapTask: Finished spill 0\n",
            "2022-03-05 15:21:48,485 INFO mapred.Task: Task:attempt_local1386624067_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-03-05 15:21:48,490 INFO mapred.LocalJobRunner: Records R/W=2682/1\n",
            "2022-03-05 15:21:48,490 INFO mapred.Task: Task 'attempt_local1386624067_0001_m_000000_0' done.\n",
            "2022-03-05 15:21:48,498 INFO mapred.Task: Final Counters for attempt_local1386624067_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1140483\n",
            "\t\tFILE: Number of bytes written=2528551\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=23305\n",
            "\t\tMap output records=203048\n",
            "\t\tMap output bytes=1502156\n",
            "\t\tMap output materialized bytes=1908258\n",
            "\t\tInput split bytes=90\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=203048\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1135453\n",
            "2022-03-05 15:21:48,498 INFO mapred.LocalJobRunner: Finishing task: attempt_local1386624067_0001_m_000000_0\n",
            "2022-03-05 15:21:48,498 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-03-05 15:21:48,503 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-03-05 15:21:48,506 INFO mapred.LocalJobRunner: Starting task: attempt_local1386624067_0001_r_000000_0\n",
            "2022-03-05 15:21:48,515 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:21:48,515 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:21:48,515 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-03-05 15:21:48,520 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5db8538b\n",
            "2022-03-05 15:21:48,522 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-03-05 15:21:48,540 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-03-05 15:21:48,545 INFO reduce.EventFetcher: attempt_local1386624067_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-03-05 15:21:48,582 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1386624067_0001_m_000000_0 decomp: 1908254 len: 1908258 to MEMORY\n",
            "2022-03-05 15:21:48,587 INFO reduce.InMemoryMapOutput: Read 1908254 bytes from map-output for attempt_local1386624067_0001_m_000000_0\n",
            "2022-03-05 15:21:48,590 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1908254, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1908254\n",
            "2022-03-05 15:21:48,595 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-03-05 15:21:48,596 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:21:48,596 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-03-05 15:21:48,635 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-03-05 15:21:48,635 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1908244 bytes\n",
            "2022-03-05 15:21:48,822 INFO reduce.MergeManagerImpl: Merged 1 segments, 1908254 bytes to disk to satisfy reduce memory limit\n",
            "2022-03-05 15:21:48,822 INFO reduce.MergeManagerImpl: Merging 1 files, 1908258 bytes from disk\n",
            "2022-03-05 15:21:48,823 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-03-05 15:21:48,823 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-03-05 15:21:48,824 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1908244 bytes\n",
            "2022-03-05 15:21:48,824 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:21:48,832 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2022-03-05 15:21:48,837 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-03-05 15:21:48,840 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-03-05 15:21:48,860 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:48,861 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:48,862 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:48,878 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:48,916 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:49,005 INFO streaming.PipeMapRed: Records R/W=18368/1\n",
            "2022-03-05 15:21:49,011 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-03-05 15:21:49,283 INFO streaming.PipeMapRed: R/W/S=100000/10812/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:49,540 INFO streaming.PipeMapRed: R/W/S=200000/21522/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:21:49,570 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-03-05 15:21:49,571 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-03-05 15:21:49,572 INFO mapred.Task: Task:attempt_local1386624067_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-03-05 15:21:49,573 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:21:49,573 INFO mapred.Task: Task attempt_local1386624067_0001_r_000000_0 is allowed to commit now\n",
            "2022-03-05 15:21:49,574 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1386624067_0001_r_000000_0' to file:/hduser/word_count/output\n",
            "2022-03-05 15:21:49,577 INFO mapred.LocalJobRunner: Records R/W=18368/1 > reduce\n",
            "2022-03-05 15:21:49,577 INFO mapred.Task: Task 'attempt_local1386624067_0001_r_000000_0' done.\n",
            "2022-03-05 15:21:49,578 INFO mapred.Task: Final Counters for attempt_local1386624067_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=4957031\n",
            "\t\tFILE: Number of bytes written=4683567\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=23005\n",
            "\t\tReduce shuffle bytes=1908258\n",
            "\t\tReduce input records=203048\n",
            "\t\tReduce output records=23005\n",
            "\t\tSpilled Records=203048\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=33\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=246758\n",
            "2022-03-05 15:21:49,580 INFO mapred.LocalJobRunner: Finishing task: attempt_local1386624067_0001_r_000000_0\n",
            "2022-03-05 15:21:49,580 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-03-05 15:21:50,012 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-03-05 15:21:50,012 INFO mapreduce.Job: Job job_local1386624067_0001 completed successfully\n",
            "2022-03-05 15:21:50,027 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6097514\n",
            "\t\tFILE: Number of bytes written=7212118\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=23305\n",
            "\t\tMap output records=203048\n",
            "\t\tMap output bytes=1502156\n",
            "\t\tMap output materialized bytes=1908258\n",
            "\t\tInput split bytes=90\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=23005\n",
            "\t\tReduce shuffle bytes=1908258\n",
            "\t\tReduce input records=203048\n",
            "\t\tReduce output records=23005\n",
            "\t\tSpilled Records=406096\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=33\n",
            "\t\tTotal committed heap usage (bytes)=698351616\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1135453\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=246758\n",
            "2022-03-05 15:21:50,031 INFO streaming.StreamJob: Output directory: /hduser/word_count/output\n"
          ]
        }
      ],
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "-mapper 'python mapper.py unigram' -reducer 'python reducer.py' \\\n",
        "-input /hduser/word_count/input \\\n",
        "-output /hduser/word_count/output \\\n",
        "-file /content/map-reduce/mapper.py -file /content/map-reduce/reducer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJZKfJKgJ_-9",
        "outputId": "9471afce-7304-4e41-d9a4-9524b7628d11",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"'About\t1\n",
            "\"'After\t1\n",
            "\"'All\t1\n",
            "\"'An\t1\n",
            "\"'And\t1\n",
            "\"'Arthur\t1\n",
            "\"'At\t2\n",
            "\"'Black\t1\n",
            "\"'But\t4\n",
            "\"'Cause\t1\n",
            "\"'Consider,\t1\n",
            "\"'Does\t1\n",
            "\"'For\t1\n",
            "\"'Friends,'\t1\n",
            "\"'God\t1\n",
            "\"'Half\t1\n",
            "\"'He\t1\n",
            "\"'Here\t1\n",
            "\"'How\t1\n",
            "\"'Hum!'\t1\n",
            "\"'I\t11\n",
            "\"'IVY\t1\n",
            "\"'If\t1\n",
            "\"'It\t6\n",
            "\"'It's\t3\n",
            "\"'Jack\t1\n",
            "\"'Listen\t1\n",
            "\"'Look\t1\n",
            "\"'Mr.\t1\n",
            "\"'My\t1\n",
            "\"'No\t2\n",
            "\"'No;\t2\n",
            "\"'None\t1\n",
            "\"'Nonsense!'\t1\n",
            "\"'Nonsense,\t1\n",
            "\"'Not\t2\n",
            "\"'Nothing\t1\n",
            "\"'On\t1\n",
            "\"'Perhaps,\t1\n",
            "\"'Populus\t1\n",
            "\"'Possibly\t1\n",
            "\"'Quite\t1\n",
            "\"'Rache,'\t1\n",
            "\"'See\t1\n",
            "\"'So\t1\n",
            "\"'Take\t1\n",
            "\"'Tention!\"\t1\n",
            "\"'The\t1\n",
            "\"'Then\t1\n",
            "\"'There\t3\n",
            "\"'This\t2\n",
            "\"'Tis\t2\n",
            "\"'To\t1\n",
            "\"'We'll\t1\n",
            "\"'Well,\t3\n",
            "\"'Well?'\t1\n",
            "\"'What\t3\n",
            "\"'When\t1\n",
            "\"'Where\t1\n",
            "\"'Who\t1\n",
            "\"'Why,\t1\n",
            "\"'Would\t1\n",
            "\"'Yes.'\t3\n",
            "\"'You\t6\n",
            "\"'Your\t2\n",
            "\"--Holmes's\t1\n",
            "\"--diverting\t1\n",
            "\"--in\t1\n",
            "\"13,\t1\n",
            "\"1742.\"\t1\n",
            "\"1884.\"\t1\n",
            "\"A\t57\n",
            "\"About\t6\n",
            "\"Absolutely\t1\n",
            "\"After\t2\n",
            "\"Ah!\t6\n",
            "\"Ah,\t26\n",
            "\"All\t12\n",
            "\"Always!\"\t1\n",
            "\"Am\t1\n",
            "\"Amen!\t1\n",
            "\"American\t1\n",
            "\"American,\t1\n",
            "\"Ames,\t1\n",
            "\"Ames,\"\t1\n",
            "\"Among\t1\n",
            "\"An\t6\n",
            "\"And\t124\n",
            "\"And,\t1\n",
            "\"Any\t4\n",
            "\"Anyone\t1\n",
            "\"Anything\t4\n",
            "\"Apart\t1\n",
            "\"Are\t10\n",
            "\"Aren't\t1\n",
            "\"Arthur\t1\n",
            "\"As\t12\n",
            "\"Ask\t1\n",
            "\"At\t10\n",
            "\"Au\t1\n",
            "\"Away,\t1\n",
            "\"Ay,\t7\n",
            "\"Ay,\"\t1\n",
            "\"Baldwin--he\t1\n",
            "\"Barrymore\t2\n",
            "\"Bartholomew\t2\n",
            "\"Baskerville\t2\n",
            "\"Be\t2\n",
            "\"Bear\t"
          ]
        }
      ],
      "source": [
        "!hadoop fs -head /hduser/word_count/output/part-00000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CtSOXvEK57O"
      },
      "source": [
        "- Modify mapper.py as follows:\n",
        "  - Transform all characters in word to lowercase\n",
        "  - Remove special character\n",
        "- Run MapReduce again. Show your result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ex8tvEMKQSJ",
        "outputId": "10323dd4-8bf0-43c8-9222-2aa370d4f65b",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-05 15:22:46,417 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/map-reduce/mapper.py, /content/map-reduce/reducer.py] [] /tmp/streamjob10552111912318699170.jar tmpDir=null\n",
            "2022-03-05 15:22:47,191 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-03-05 15:22:47,351 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-03-05 15:22:47,351 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-03-05 15:22:47,379 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-03-05 15:22:47,547 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-03-05 15:22:47,566 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-03-05 15:22:47,902 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local890106340_0001\n",
            "2022-03-05 15:22:47,902 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-03-05 15:22:48,279 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local890106340_0001_333b34c8-4cb4-46d1-b42a-fb3e54f60bdc/mapper.py\n",
            "2022-03-05 15:22:48,314 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local890106340_0001_2c2aa967-bf35-489d-84ea-0c78c4ac2eed/reducer.py\n",
            "2022-03-05 15:22:48,423 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-03-05 15:22:48,424 INFO mapreduce.Job: Running job: job_local890106340_0001\n",
            "2022-03-05 15:22:48,433 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-03-05 15:22:48,435 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-03-05 15:22:48,442 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:22:48,442 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:22:48,492 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-03-05 15:22:48,495 INFO mapred.LocalJobRunner: Starting task: attempt_local890106340_0001_m_000000_0\n",
            "2022-03-05 15:22:48,526 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:22:48,528 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:22:48,559 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-03-05 15:22:48,572 INFO mapred.MapTask: Processing split: file:/hduser/word_count/input/text.txt:0+1126637\n",
            "2022-03-05 15:22:48,595 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-03-05 15:22:48,670 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-03-05 15:22:48,670 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-03-05 15:22:48,670 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-03-05 15:22:48,670 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-03-05 15:22:48,670 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-03-05 15:22:48,674 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-03-05 15:22:48,682 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py, unigram, --clean]\n",
            "2022-03-05 15:22:48,693 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-03-05 15:22:48,693 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-03-05 15:22:48,694 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-03-05 15:22:48,695 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-03-05 15:22:48,695 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-03-05 15:22:48,696 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-03-05 15:22:48,696 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-03-05 15:22:48,696 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-03-05 15:22:48,697 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-03-05 15:22:48,698 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-03-05 15:22:48,698 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-03-05 15:22:48,699 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-03-05 15:22:48,732 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:48,732 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:48,734 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:48,749 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:48,843 INFO streaming.PipeMapRed: Records R/W=2682/1\n",
            "2022-03-05 15:22:49,061 INFO streaming.PipeMapRed: R/W/S=10000/52184/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:49,390 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-03-05 15:22:49,391 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-03-05 15:22:49,394 INFO mapred.LocalJobRunner: \n",
            "2022-03-05 15:22:49,394 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-03-05 15:22:49,394 INFO mapred.MapTask: Spilling map output\n",
            "2022-03-05 15:22:49,394 INFO mapred.MapTask: bufstart = 0; bufend = 1470686; bufvoid = 104857600\n",
            "2022-03-05 15:22:49,394 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25386876(101547504); length = 827521/6553600\n",
            "2022-03-05 15:22:49,432 INFO mapreduce.Job: Job job_local890106340_0001 running in uber mode : false\n",
            "2022-03-05 15:22:49,433 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-03-05 15:22:49,943 INFO mapred.MapTask: Finished spill 0\n",
            "2022-03-05 15:22:49,958 INFO mapred.Task: Task:attempt_local890106340_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-03-05 15:22:49,960 INFO mapred.LocalJobRunner: Records R/W=2682/1\n",
            "2022-03-05 15:22:49,961 INFO mapred.Task: Task 'attempt_local890106340_0001_m_000000_0' done.\n",
            "2022-03-05 15:22:49,968 INFO mapred.Task: Final Counters for attempt_local890106340_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1140483\n",
            "\t\tFILE: Number of bytes written=2501787\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=23305\n",
            "\t\tMap output records=206881\n",
            "\t\tMap output bytes=1470686\n",
            "\t\tMap output materialized bytes=1884454\n",
            "\t\tInput split bytes=90\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=206881\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=368050176\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1135453\n",
            "2022-03-05 15:22:49,968 INFO mapred.LocalJobRunner: Finishing task: attempt_local890106340_0001_m_000000_0\n",
            "2022-03-05 15:22:49,970 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-03-05 15:22:49,973 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-03-05 15:22:49,974 INFO mapred.LocalJobRunner: Starting task: attempt_local890106340_0001_r_000000_0\n",
            "2022-03-05 15:22:49,986 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:22:49,991 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:22:49,991 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-03-05 15:22:49,996 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1101578c\n",
            "2022-03-05 15:22:49,998 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-03-05 15:22:50,022 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-03-05 15:22:50,041 INFO reduce.EventFetcher: attempt_local890106340_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-03-05 15:22:50,081 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local890106340_0001_m_000000_0 decomp: 1884450 len: 1884454 to MEMORY\n",
            "2022-03-05 15:22:50,086 INFO reduce.InMemoryMapOutput: Read 1884450 bytes from map-output for attempt_local890106340_0001_m_000000_0\n",
            "2022-03-05 15:22:50,087 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1884450, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1884450\n",
            "2022-03-05 15:22:50,091 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-03-05 15:22:50,093 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:22:50,093 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-03-05 15:22:50,100 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-03-05 15:22:50,103 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1884446 bytes\n",
            "2022-03-05 15:22:50,323 INFO reduce.MergeManagerImpl: Merged 1 segments, 1884450 bytes to disk to satisfy reduce memory limit\n",
            "2022-03-05 15:22:50,329 INFO reduce.MergeManagerImpl: Merging 1 files, 1884454 bytes from disk\n",
            "2022-03-05 15:22:50,330 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-03-05 15:22:50,330 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-03-05 15:22:50,331 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1884446 bytes\n",
            "2022-03-05 15:22:50,332 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:22:50,347 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2022-03-05 15:22:50,354 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-03-05 15:22:50,356 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-03-05 15:22:50,378 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:50,378 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:50,380 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:50,396 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:50,436 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-03-05 15:22:50,441 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:50,585 INFO streaming.PipeMapRed: Records R/W=36941/1\n",
            "2022-03-05 15:22:50,798 INFO streaming.PipeMapRed: R/W/S=100000/4746/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:51,008 INFO streaming.PipeMapRed: R/W/S=200000/10322/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:22:51,040 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-03-05 15:22:51,041 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-03-05 15:22:51,042 INFO mapred.Task: Task:attempt_local890106340_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-03-05 15:22:51,043 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:22:51,043 INFO mapred.Task: Task attempt_local890106340_0001_r_000000_0 is allowed to commit now\n",
            "2022-03-05 15:22:51,045 INFO output.FileOutputCommitter: Saved output of task 'attempt_local890106340_0001_r_000000_0' to file:/hduser/word_count/output_q3\n",
            "2022-03-05 15:22:51,047 INFO mapred.LocalJobRunner: Records R/W=36941/1 > reduce\n",
            "2022-03-05 15:22:51,047 INFO mapred.Task: Task 'attempt_local890106340_0001_r_000000_0' done.\n",
            "2022-03-05 15:22:51,048 INFO mapred.Task: Final Counters for attempt_local890106340_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=4909423\n",
            "\t\tFILE: Number of bytes written=4508944\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=11830\n",
            "\t\tReduce shuffle bytes=1884454\n",
            "\t\tReduce input records=206881\n",
            "\t\tReduce output records=11830\n",
            "\t\tSpilled Records=206881\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=368050176\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=122703\n",
            "2022-03-05 15:22:51,048 INFO mapred.LocalJobRunner: Finishing task: attempt_local890106340_0001_r_000000_0\n",
            "2022-03-05 15:22:51,048 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-03-05 15:22:51,437 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-03-05 15:22:51,438 INFO mapreduce.Job: Job job_local890106340_0001 completed successfully\n",
            "2022-03-05 15:22:51,447 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6049906\n",
            "\t\tFILE: Number of bytes written=7010731\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=23305\n",
            "\t\tMap output records=206881\n",
            "\t\tMap output bytes=1470686\n",
            "\t\tMap output materialized bytes=1884454\n",
            "\t\tInput split bytes=90\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=11830\n",
            "\t\tReduce shuffle bytes=1884454\n",
            "\t\tReduce input records=206881\n",
            "\t\tReduce output records=11830\n",
            "\t\tSpilled Records=413762\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=736100352\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1135453\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=122703\n",
            "2022-03-05 15:22:51,447 INFO streaming.StreamJob: Output directory: /hduser/word_count/output_q3\n"
          ]
        }
      ],
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "-mapper 'python mapper.py unigram --clean' -reducer 'python reducer.py' \\\n",
        "-input /hduser/word_count/input \\\n",
        "-output /hduser/word_count/output_q3 \\\n",
        "-file /content/map-reduce/mapper.py -file /content/map-reduce/reducer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39S2TSQBM0mO",
        "outputId": "8c7d2237-fe35-44fd-fb04-6b9fc71ae4ce",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t6\n",
            "10\t5\n",
            "109\t1\n",
            "11\t5\n",
            "12\t3\n",
            "127\t1\n",
            "129\t1\n",
            "13\t7\n",
            "13th\t1\n",
            "14\t3\n",
            "14th\t1\n",
            "15\t4\n",
            "1543\t1\n",
            "15th\t1\n",
            "16\t1\n",
            "1642\t1\n",
            "1644\t1\n",
            "1647\t1\n",
            "16th\t1\n",
            "17\t1\n",
            "171\t1\n",
            "1730\t1\n",
            "1742\t2\n",
            "1750\t2\n",
            "17th\t1\n",
            "1800\t1\n",
            "1857\t1\n",
            "1860\t1\n",
            "1865\t1\n",
            "1871\t1\n",
            "1872\t1\n",
            "1874\t1\n",
            "1875\t2\n",
            "1876\t1\n",
            "1878\t3\n",
            "1882\t6\n",
            "1883\t1\n",
            "1884\t2\n",
            "18th\t1\n",
            "19\t1\n",
            "2\t8\n",
            "20\t1\n",
            "200\t1\n",
            "21\t2\n",
            "22\t1\n",
            "221b\t4\n",
            "23\t1\n",
            "24\t2\n",
            "249\t1\n",
            "25\t1\n",
            "26\t2\n",
            "27\t2\n",
            "2704\t3\n",
            "28\t1\n",
            "28th\t1\n",
            "29\t4\n",
            "293\t1\n",
            "3\t14\n",
            "30\t2\n",
            "31\t1\n",
            "34\t1\n",
            "340\t1\n",
            "341\t9\n",
            "34th\t2\n",
            "36\t1\n",
            "37\t2\n",
            "3d\t3\n",
            "4\t7\n",
            "41\t1\n",
            "46\t2\n",
            "47\t1\n",
            "4th\t5\n",
            "5\t7\n",
            "534\t8\n",
            "6\t6\n",
            "66\t1\n",
            "6th\t1\n",
            "7\t8\n",
            "76\t1\n",
            "8\t5\n",
            "80\t1\n",
            "9\t6\n",
            "97163\t1\n",
            "a\t4737\n",
            "aback\t2\n",
            "abandon\t7\n",
            "abandoned\t7\n",
            "abandoning\t1\n",
            "abandons\t1\n",
            "abashed\t1\n",
            "abdullah\t8\n",
            "abelwhite\t4\n",
            "aberdeen\t1\n",
            "aberdonian\t2\n",
            "abetting\t1\n",
            "abhor\t2\n",
            "abiding\t2\n",
            "abilities\t1\n",
            "ability\t2\n",
            "able\t73\n",
            "aboard\t4\n",
            "abode\t5\n",
            "aborigines\t1\n",
            "abortive\t1\n",
            "about\t355\n",
            "above\t53\n",
            "abreast\t1\n",
            "abroad\t2\n",
            "abrupt\t1\n",
            "abruptly\t2\n",
            "absence\t13\n",
            "absent\t14\n",
            "absentee\t1\n",
            "absolute\t6\n",
            "absolutely\t9\n",
            "absorb\t1\n",
            "absorbed\t12\n",
            "absorbing\t4\n",
            "abstainers\t1\n",
            "abstract\t2\n",
            "abstracted\t3\n",
            "abstraction\t1\n",
            "abstruse\t2\n",
            "absurd\t5\n",
            "absurdity\t1\n",
            "absurdly\t1\n",
            "abused\t2\n",
            "abusing\t1\n",
            "abyss\t1\n",
            "ac\t1\n",
            "accent\t6\n",
            "accept\t9\n",
            "accepted\t4\n",
            "access\t2\n",
            "accessory\t2\n",
            "a"
          ]
        }
      ],
      "source": [
        "!hadoop fs -head /hduser/word_count/output_q3/part-00000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Foy-4Ligi0"
      },
      "source": [
        "- How many times that the word `data` appears?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icUAbud6if5N",
        "outputId": "a956130f-aa7d-4ad6-8a6b-9473533e7d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\t8\n"
          ]
        }
      ],
      "source": [
        "!hadoop fs -cat /hduser/word_count/output_q3/part-00000 | grep \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7JItps-EB8P"
      },
      "source": [
        "> **Observation**\n",
        "> - The word `data` appeared in the text for `8` times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x3JaYsBjAPW"
      },
      "source": [
        "- Modify mapper.py to capture the combination of 2 neighbouring words (Bigram)\n",
        "- Run MapReduce again. Show your result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef51lWQNiWU2",
        "outputId": "c4907dff-9b1f-4696-f244-c36cc9d5fcf9",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-05 15:23:49,580 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/map-reduce/mapper.py, /content/map-reduce/reducer.py] [] /tmp/streamjob12880654836743515499.jar tmpDir=null\n",
            "2022-03-05 15:23:50,311 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-03-05 15:23:50,530 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-03-05 15:23:50,530 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-03-05 15:23:50,548 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-03-05 15:23:50,705 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-03-05 15:23:50,725 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-03-05 15:23:51,016 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local919724858_0001\n",
            "2022-03-05 15:23:51,016 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-03-05 15:23:51,386 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local919724858_0001_d1161968-55af-4c56-a629-b8b50df181ba/mapper.py\n",
            "2022-03-05 15:23:51,433 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local919724858_0001_45241db5-37c9-4fd9-afba-9e33f17154cf/reducer.py\n",
            "2022-03-05 15:23:51,530 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-03-05 15:23:51,532 INFO mapreduce.Job: Running job: job_local919724858_0001\n",
            "2022-03-05 15:23:51,537 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-03-05 15:23:51,540 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-03-05 15:23:51,544 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:23:51,544 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:23:51,584 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-03-05 15:23:51,587 INFO mapred.LocalJobRunner: Starting task: attempt_local919724858_0001_m_000000_0\n",
            "2022-03-05 15:23:51,613 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:23:51,615 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:23:51,652 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-03-05 15:23:51,667 INFO mapred.MapTask: Processing split: file:/hduser/word_count/input/text.txt:0+1126637\n",
            "2022-03-05 15:23:51,690 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-03-05 15:23:51,770 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-03-05 15:23:51,770 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-03-05 15:23:51,770 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-03-05 15:23:51,770 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-03-05 15:23:51,770 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-03-05 15:23:51,775 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-03-05 15:23:51,788 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py, bigram, --clean]\n",
            "2022-03-05 15:23:51,803 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-03-05 15:23:51,804 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-03-05 15:23:51,805 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-03-05 15:23:51,805 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-03-05 15:23:51,806 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-03-05 15:23:51,806 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-03-05 15:23:51,807 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-03-05 15:23:51,807 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-03-05 15:23:51,807 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-03-05 15:23:51,808 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-03-05 15:23:51,808 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-03-05 15:23:51,809 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-03-05 15:23:51,839 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:51,839 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:51,841 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:51,865 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:51,952 INFO streaming.PipeMapRed: Records R/W=2682/1\n",
            "2022-03-05 15:23:52,165 INFO streaming.PipeMapRed: R/W/S=10000/53092/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:52,523 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-03-05 15:23:52,524 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-03-05 15:23:52,527 INFO mapred.LocalJobRunner: \n",
            "2022-03-05 15:23:52,528 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-03-05 15:23:52,528 INFO mapred.MapTask: Spilling map output\n",
            "2022-03-05 15:23:52,528 INFO mapred.MapTask: bufstart = 0; bufend = 2288496; bufvoid = 104857600\n",
            "2022-03-05 15:23:52,528 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25460408(101841632); length = 753989/6553600\n",
            "2022-03-05 15:23:52,536 INFO mapreduce.Job: Job job_local919724858_0001 running in uber mode : false\n",
            "2022-03-05 15:23:52,537 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-03-05 15:23:53,213 INFO mapred.MapTask: Finished spill 0\n",
            "2022-03-05 15:23:53,228 INFO mapred.Task: Task:attempt_local919724858_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-03-05 15:23:53,230 INFO mapred.LocalJobRunner: Records R/W=2682/1\n",
            "2022-03-05 15:23:53,230 INFO mapred.Task: Task 'attempt_local919724858_0001_m_000000_0' done.\n",
            "2022-03-05 15:23:53,237 INFO mapred.Task: Final Counters for attempt_local919724858_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1140483\n",
            "\t\tFILE: Number of bytes written=3282829\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=23305\n",
            "\t\tMap output records=188498\n",
            "\t\tMap output bytes=2288496\n",
            "\t\tMap output materialized bytes=2665498\n",
            "\t\tInput split bytes=90\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=188498\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=361758720\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1135453\n",
            "2022-03-05 15:23:53,237 INFO mapred.LocalJobRunner: Finishing task: attempt_local919724858_0001_m_000000_0\n",
            "2022-03-05 15:23:53,237 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-03-05 15:23:53,242 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-03-05 15:23:53,242 INFO mapred.LocalJobRunner: Starting task: attempt_local919724858_0001_r_000000_0\n",
            "2022-03-05 15:23:53,281 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-03-05 15:23:53,281 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-03-05 15:23:53,281 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-03-05 15:23:53,286 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2712a54\n",
            "2022-03-05 15:23:53,288 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-03-05 15:23:53,308 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-03-05 15:23:53,322 INFO reduce.EventFetcher: attempt_local919724858_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-03-05 15:23:53,349 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local919724858_0001_m_000000_0 decomp: 2665494 len: 2665498 to MEMORY\n",
            "2022-03-05 15:23:53,355 INFO reduce.InMemoryMapOutput: Read 2665494 bytes from map-output for attempt_local919724858_0001_m_000000_0\n",
            "2022-03-05 15:23:53,357 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2665494, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2665494\n",
            "2022-03-05 15:23:53,360 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-03-05 15:23:53,361 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:23:53,361 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-03-05 15:23:53,367 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-03-05 15:23:53,367 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2665480 bytes\n",
            "2022-03-05 15:23:53,539 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-03-05 15:23:53,545 INFO reduce.MergeManagerImpl: Merged 1 segments, 2665494 bytes to disk to satisfy reduce memory limit\n",
            "2022-03-05 15:23:53,546 INFO reduce.MergeManagerImpl: Merging 1 files, 2665498 bytes from disk\n",
            "2022-03-05 15:23:53,546 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-03-05 15:23:53,546 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-03-05 15:23:53,547 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2665480 bytes\n",
            "2022-03-05 15:23:53,548 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:23:53,565 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2022-03-05 15:23:53,572 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-03-05 15:23:53,574 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-03-05 15:23:53,587 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:53,587 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:53,588 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:53,602 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:53,631 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:53,724 INFO streaming.PipeMapRed: Records R/W=11305/1\n",
            "2022-03-05 15:23:54,174 INFO streaming.PipeMapRed: R/W/S=100000/40463/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-03-05 15:23:54,408 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-03-05 15:23:54,408 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-03-05 15:23:54,411 INFO mapred.Task: Task:attempt_local919724858_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-03-05 15:23:54,413 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-03-05 15:23:54,413 INFO mapred.Task: Task attempt_local919724858_0001_r_000000_0 is allowed to commit now\n",
            "2022-03-05 15:23:54,415 INFO output.FileOutputCommitter: Saved output of task 'attempt_local919724858_0001_r_000000_0' to file:/hduser/word_count/output_q7\n",
            "2022-03-05 15:23:54,424 INFO mapred.LocalJobRunner: Records R/W=11305/1 > reduce\n",
            "2022-03-05 15:23:54,424 INFO mapred.Task: Task 'attempt_local919724858_0001_r_000000_0' done.\n",
            "2022-03-05 15:23:54,425 INFO mapred.Task: Final Counters for attempt_local919724858_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6471511\n",
            "\t\tFILE: Number of bytes written=7075228\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=81254\n",
            "\t\tReduce shuffle bytes=2665498\n",
            "\t\tReduce input records=188498\n",
            "\t\tReduce output records=81254\n",
            "\t\tSpilled Records=188498\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=361758720\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1126901\n",
            "2022-03-05 15:23:54,425 INFO mapred.LocalJobRunner: Finishing task: attempt_local919724858_0001_r_000000_0\n",
            "2022-03-05 15:23:54,425 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-03-05 15:23:54,539 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-03-05 15:23:54,540 INFO mapreduce.Job: Job job_local919724858_0001 completed successfully\n",
            "2022-03-05 15:23:54,555 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=7611994\n",
            "\t\tFILE: Number of bytes written=10358057\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=23305\n",
            "\t\tMap output records=188498\n",
            "\t\tMap output bytes=2288496\n",
            "\t\tMap output materialized bytes=2665498\n",
            "\t\tInput split bytes=90\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=81254\n",
            "\t\tReduce shuffle bytes=2665498\n",
            "\t\tReduce input records=188498\n",
            "\t\tReduce output records=81254\n",
            "\t\tSpilled Records=376996\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=723517440\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1135453\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1126901\n",
            "2022-03-05 15:23:54,558 INFO streaming.StreamJob: Output directory: /hduser/word_count/output_q7\n"
          ]
        }
      ],
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "-mapper 'python mapper.py bigram --clean' -reducer 'python reducer.py' \\\n",
        "-input /hduser/word_count/input \\\n",
        "-output /hduser/word_count/output_q7 \\\n",
        "-file /content/map-reduce/mapper.py -file /content/map-reduce/reducer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "youFxPfhkfc-",
        "outputId": "66083d82-f810-4df7-9365-994907afce21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-knowledge\t1\n",
            "1-mr\t1\n",
            "1-the\t3\n",
            "10-30\t1\n",
            "10-and\t1\n",
            "10-extract\t1\n",
            "10-plays\t1\n",
            "109-293\t1\n",
            "11-he\t1\n",
            "11-in\t1\n",
            "11-is\t1\n",
            "11-the\t1\n",
            "12-death\t1\n",
            "12-has\t1\n",
            "127-36\t1\n",
            "129-camberwell\t1\n",
            "13-127\t1\n",
            "13-duncan\t2\n",
            "13-fixing\t1\n",
            "13-he\t1\n",
            "14-the\t1\n",
            "14th-of\t1\n",
            "15-a\t1\n",
            "15-and\t1\n",
            "15-why\t1\n",
            "1543-and\t1\n",
            "16-cushion\t1\n",
            "1642-charles\t1\n",
            "1644-of\t1\n",
            "1647-are\t1\n",
            "16th-a\t1\n",
            "17-21\t1\n",
            "1742-dr\t1\n",
            "1750-and\t1\n",
            "1750-or\t1\n",
            "17th-all\t1\n",
            "1800-i\t1\n",
            "1857-and\t1\n",
            "1865-a\t1\n",
            "1871-which\t1\n",
            "1875-it\t1\n",
            "1876-of\t1\n",
            "1878-i\t1\n",
            "1878-my\t1\n",
            "1878-nearly\t1\n",
            "1882-an\t1\n",
            "1882-do\t1\n",
            "1882-grimpen\t1\n",
            "1882-my\t1\n",
            "1882-to\t1\n",
            "1883-medical\t1\n",
            "1884-at\t1\n",
            "1884-it\t1\n",
            "18th-of\t1\n",
            "19-the\t1\n",
            "2-philosophy\t1\n",
            "2-sherlock\t1\n",
            "2-the\t3\n",
            "2-upon\t1\n",
            "200-pounds\t1\n",
            "21-41\t1\n",
            "21-cold\t1\n",
            "221b-baker\t2\n",
            "23-he\t1\n",
            "24-1872\t1\n",
            "25-and\t1\n",
            "26-fancy\t1\n",
            "27-as\t1\n",
            "27-had\t1\n",
            "2704-and\t1\n",
            "2704-is\t1\n",
            "2704-said\t1\n",
            "28-to\t1\n",
            "28th-of\t1\n",
            "29-but\t1\n",
            "29-chicago\t3\n",
            "293-5\t1\n",
            "3-37\t1\n",
            "3-astronomy\t1\n",
            "3-before\t1\n",
            "3-i\t1\n",
            "3-lauriston\t3\n",
            "3-lodge\t1\n",
            "3-mayfield\t1\n",
            "3-pinchin\t1\n",
            "3-the\t2\n",
            "3-turpey\t1\n",
            "30-and\t1\n",
            "30-train\t1\n",
            "31-4\t1\n",
            "34-do\t1\n",
            "340-miles\t1\n",
            "341-i\t1\n",
            "341-it\t1\n",
            "341-vermissa\t4\n",
            "341-were\t1\n",
            "341-whatever\t1\n",
            "34th-bombay\t1\n",
            "36-31\t1"
          ]
        }
      ],
      "source": [
        "!hadoop fs -head /hduser/word_count/output_q7/part-00000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLXBol3bkSrO"
      },
      "source": [
        "- How many times that `sherlock` and `holmes` appears together?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo54Ecotj2Oc",
        "outputId": "7d58d1d0-bd4f-43d0-b0f5-4bf9a4d9be8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sherlock-holmes\t115\n"
          ]
        }
      ],
      "source": [
        "!hadoop fs -cat /hduser/word_count/output_q7/part-00000 | grep \"sherlock-holmes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D-5D0NB8EwlW"
      },
      "outputs": [],
      "source": [
        "!hadoop fs -cat /hduser/word_count/output_q7/part-00000 | grep \"holmes-sherlock\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo4fGHpjEeNM"
      },
      "source": [
        "> **Observation**\n",
        "> - The words `sherlock`and `holmes` appeared together in the text for `115` times"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hadoop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}