{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce\n",
    "- The purpose is to practice using hadoop software framework by executing map reduce tasks that count words and bigrams in Sherlock Holmes book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eeq_5gP_vjk"
   },
   "source": [
    "### Install Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_TNFkoZrSKt",
    "outputId": "81117684-5c38-4078-cdac-201b5107f61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-01 15:13:25--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
      "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
      "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 500749234 (478M) [application/x-gzip]\n",
      "Saving to: ‘hadoop-3.3.0.tar.gz’\n",
      "\n",
      "hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  19.1MB/s    in 26s     \n",
      "\n",
      "2022-03-01 15:13:52 (18.4 MB/s) - ‘hadoop-3.3.0.tar.gz’ saved [500749234/500749234]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oECBj-r-rYcw"
   },
   "outputs": [],
   "source": [
    "!tar -xzf hadoop-3.3.0.tar.gz\n",
    "!cp -r hadoop-3.3.0/ /usr/local/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCzkbGBkAOc1"
   },
   "source": [
    "### Set up path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K9q6Jv6suzfA"
   },
   "outputs": [],
   "source": [
    "!echo \"export JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:bin/java::\")\" >> \\\n",
    "/usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UPesx3fy0iwG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] += ':/usr/local/hadoop-3.3.0/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBgYMO4qAUuY"
   },
   "source": [
    "### Run Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMMqS3M_2QRb",
    "outputId": "17867389-d7d3-4f86-834c-01ea81194e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
      "  where CLASSNAME is a user-provided Java class\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "buildpaths                       attempt to add class files from build tree\n",
      "--config dir                     Hadoop config directory\n",
      "--debug                          turn on shell script debug mode\n",
      "--help                           usage information\n",
      "hostnames list[,of,host,names]   hosts to use in slave mode\n",
      "hosts filename                   list of hosts to use in slave mode\n",
      "loglevel level                   set the log4j level for this command\n",
      "workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "daemonlog     get/set the log level for each daemon\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "archive       create a Hadoop archive\n",
      "checknative   check native Hadoop and compression libraries availability\n",
      "classpath     prints the class path needed to get the Hadoop jar and the\n",
      "              required libraries\n",
      "conftest      validate configuration XML files\n",
      "credential    interact with credential providers\n",
      "distch        distributed metadata changer\n",
      "distcp        copy file or directories recursively\n",
      "dtutil        operations related to delegation tokens\n",
      "envvars       display computed Hadoop environment variables\n",
      "fs            run a generic filesystem user client\n",
      "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
      "              production load\n",
      "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
      "              applications, not this command.\n",
      "jnipath       prints the java.library.path\n",
      "kdiag         Diagnose Kerberos Problems\n",
      "kerbname      show auth_to_local principal conversion\n",
      "key           manage keys via the KeyProvider\n",
      "rumenfolder   scale a rumen input trace\n",
      "rumentrace    convert logs into a rumen trace\n",
      "s3guard       manage metadata on S3\n",
      "trace         view and modify Hadoop tracing settings\n",
      "version       print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "kms           run KMS, the Key Management Server\n",
      "registrydns   run the registry DNS server\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "!hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5ybGPzeAYU3"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yBaj98y-MaI",
    "outputId": "7dfa7177-110c-4b57-eb89-1e90066f75c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=130pPxku3vSEOvJb-Wsx1ysF0Fz7WTeKo\n",
      "To: /content/sherlock_text.txt\n",
      "100% 1.13M/1.13M [00:00<00:00, 6.45MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown -O sherlock_text.txt --id 130pPxku3vSEOvJb-Wsx1ysF0Fz7WTeKo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVSJ01jef9UL"
   },
   "source": [
    "### Download scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZODpkuAhf85Q",
    "outputId": "69886544-3985-4ca0-b74e-b3f33bbfbfea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'map-reduce'...\n",
      "remote: Enumerating objects: 23, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 23 (delta 9), reused 15 (delta 5), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (23/23), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ppkgtmm/big-data-map-reduce.git map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noD4SFDkgPSI",
    "outputId": "f08efb14-237b-43f4-81d8-9c121ec67533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage: python3 file_name.py count_type [clean]\n",
      "\n",
      "        count_type\tunigram or bigram\n",
      "        clean\t\tspecify t if you want to clean texts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 map-reduce/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNh_lhbU_oiv"
   },
   "source": [
    "### Steps\n",
    "- Copy file from local machine to HDFS. Show your command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZzbYVci-wO6",
    "outputId": "a95671c8-7e89-4e15-bfec-f556b475875c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root root    1126637 2022-03-01 15:14 /hduser/word_count/input/text.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir /hduser\n",
    "!hadoop fs -mkdir /hduser/word_count\n",
    "!hadoop fs -mkdir /hduser/word_count/input\n",
    "!hadoop fs -put ./sherlock_text.txt /hduser/word_count/input/text.txt\n",
    "!hadoop fs -ls /hduser/word_count/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P518kRCd-zPJ",
    "outputId": "6c702e9a-c563-4855-beb8-60512a55b080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "A STUDY IN SCARLET.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "PART I.\r\n",
      "\r\n",
      "(_Being a reprint from the reminiscences of_ JOHN H. WATSON, M.D., _late\r\n",
      "of the Army Medical Department._) [2]\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "CHAPTER I. MR. SHERLOCK HOLMES.\r\n",
      "\r\n",
      "\r\n",
      "IN the year 1878 I took my degree of Doctor of Medicine of the\r\n",
      "University of London, and proceeded to Netley to go through the course\r\n",
      "prescribed for surgeons in the army. Having completed my studies there,\r\n",
      "I was duly attached to the Fifth Northumberland Fusiliers as Assistant\r\n",
      "Surgeon. The regiment was stationed in India at the time, and before\r\n",
      "I could join it, the second Afghan war had broken out. On landing at\r\n",
      "Bombay, I learned that my corps had advanced through the passes, and\r\n",
      "was already deep in the enemy's country. I followed, however, with many\r\n",
      "other officers who were in the same situation as myself, and succeeded\r\n",
      "in reaching Candahar in safety, where I found my regiment, and at once\r\n",
      "entered upon my new duties.\r\n",
      "\r\n",
      "The campaign brought honours and promotion to many, but for me it had\r\n",
      "nothing but "
     ]
    }
   ],
   "source": [
    "!hadoop fs -head /hduser/word_count/input/text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKWnjxJUAqh6"
   },
   "source": [
    "- Run MapReduce through Hadoop Streaming. Show your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqqDAisjD77Y",
    "outputId": "f4847c72-9af1-46a8-d06b-5dc4b8c1a3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t1\n",
      "STUDY\t1\n",
      "IN\t1\n",
      "SCARLET.\t1\n",
      "PART\t1\n",
      "I.\t1\n",
      "(_Being\t1\n",
      "a\t1\n",
      "reprint\t1\n",
      "from\t1\n",
      "the\t1\n",
      "reminiscences\t1\n",
      "of_\t1\n",
      "JOHN\t1\n",
      "H.\t1\n",
      "WATSON,\t1\n",
      "M.D.,\t1\n",
      "_late\t1\n"
     ]
    }
   ],
   "source": [
    "!head /content/sherlock_text.txt | python3 /content/map-reduce/mapper.py unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0A7HNOAEWfA",
    "outputId": "885d402b-ec0c-4ecf-f670-695a0c050b47",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 15:15:06,665 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/content/map-reduce/mapper.py, /content/map-reduce/reducer.py] [] /tmp/streamjob11337564572291151494.jar tmpDir=null\n",
      "2022-03-01 15:15:07,408 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-03-01 15:15:07,649 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-03-01 15:15:07,650 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2022-03-01 15:15:07,675 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2022-03-01 15:15:07,902 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-03-01 15:15:07,923 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2022-03-01 15:15:08,323 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1969085402_0001\n",
      "2022-03-01 15:15:08,324 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-03-01 15:15:08,714 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1969085402_0001_7fffffe7-86e1-4527-aea6-be1a7e33459d/mapper.py\n",
      "2022-03-01 15:15:08,766 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1969085402_0001_d9b415fb-06ac-46e1-8b34-e97badb7c0b9/reducer.py\n",
      "2022-03-01 15:15:08,866 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2022-03-01 15:15:08,867 INFO mapreduce.Job: Running job: job_local1969085402_0001\n",
      "2022-03-01 15:15:08,873 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2022-03-01 15:15:08,875 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2022-03-01 15:15:08,880 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:15:08,880 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:15:08,935 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2022-03-01 15:15:08,939 INFO mapred.LocalJobRunner: Starting task: attempt_local1969085402_0001_m_000000_0\n",
      "2022-03-01 15:15:08,982 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:15:08,982 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:15:09,031 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2022-03-01 15:15:09,047 INFO mapred.MapTask: Processing split: file:/hduser/word_count/input/text.txt:0+1126637\n",
      "2022-03-01 15:15:09,073 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2022-03-01 15:15:09,147 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2022-03-01 15:15:09,147 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2022-03-01 15:15:09,147 INFO mapred.MapTask: soft limit at 83886080\n",
      "2022-03-01 15:15:09,147 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2022-03-01 15:15:09,147 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2022-03-01 15:15:09,151 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2022-03-01 15:15:09,162 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py, unigram]\n",
      "2022-03-01 15:15:09,171 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2022-03-01 15:15:09,172 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2022-03-01 15:15:09,172 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2022-03-01 15:15:09,173 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2022-03-01 15:15:09,174 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2022-03-01 15:15:09,174 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2022-03-01 15:15:09,174 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2022-03-01 15:15:09,175 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2022-03-01 15:15:09,175 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2022-03-01 15:15:09,176 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2022-03-01 15:15:09,176 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2022-03-01 15:15:09,177 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2022-03-01 15:15:09,196 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:09,196 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:09,198 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:09,206 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:09,322 INFO streaming.PipeMapRed: Records R/W=2682/1\n",
      "2022-03-01 15:15:09,485 INFO streaming.PipeMapRed: R/W/S=10000/48764/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:09,769 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2022-03-01 15:15:09,770 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2022-03-01 15:15:09,777 INFO mapred.LocalJobRunner: \n",
      "2022-03-01 15:15:09,777 INFO mapred.MapTask: Starting flush of map output\n",
      "2022-03-01 15:15:09,777 INFO mapred.MapTask: Spilling map output\n",
      "2022-03-01 15:15:09,777 INFO mapred.MapTask: bufstart = 0; bufend = 1502156; bufvoid = 104857600\n",
      "2022-03-01 15:15:09,777 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25402208(101608832); length = 812189/6553600\n",
      "2022-03-01 15:15:09,871 INFO mapreduce.Job: Job job_local1969085402_0001 running in uber mode : false\n",
      "2022-03-01 15:15:09,873 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-03-01 15:15:10,328 INFO mapred.MapTask: Finished spill 0\n",
      "2022-03-01 15:15:10,342 INFO mapred.Task: Task:attempt_local1969085402_0001_m_000000_0 is done. And is in the process of committing\n",
      "2022-03-01 15:15:10,348 INFO mapred.LocalJobRunner: Records R/W=2682/1\n",
      "2022-03-01 15:15:10,348 INFO mapred.Task: Task 'attempt_local1969085402_0001_m_000000_0' done.\n",
      "2022-03-01 15:15:10,356 INFO mapred.Task: Final Counters for attempt_local1969085402_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1140179\n",
      "\t\tFILE: Number of bytes written=2528241\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=23305\n",
      "\t\tMap output records=203048\n",
      "\t\tMap output bytes=1502156\n",
      "\t\tMap output materialized bytes=1908258\n",
      "\t\tInput split bytes=90\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=203048\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=362807296\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1135453\n",
      "2022-03-01 15:15:10,356 INFO mapred.LocalJobRunner: Finishing task: attempt_local1969085402_0001_m_000000_0\n",
      "2022-03-01 15:15:10,358 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2022-03-01 15:15:10,363 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2022-03-01 15:15:10,363 INFO mapred.LocalJobRunner: Starting task: attempt_local1969085402_0001_r_000000_0\n",
      "2022-03-01 15:15:10,375 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:15:10,375 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:15:10,375 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2022-03-01 15:15:10,378 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f1310ca\n",
      "2022-03-01 15:15:10,380 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2022-03-01 15:15:10,401 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2022-03-01 15:15:10,403 INFO reduce.EventFetcher: attempt_local1969085402_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2022-03-01 15:15:10,494 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1969085402_0001_m_000000_0 decomp: 1908254 len: 1908258 to MEMORY\n",
      "2022-03-01 15:15:10,501 INFO reduce.InMemoryMapOutput: Read 1908254 bytes from map-output for attempt_local1969085402_0001_m_000000_0\n",
      "2022-03-01 15:15:10,505 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1908254, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1908254\n",
      "2022-03-01 15:15:10,510 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2022-03-01 15:15:10,511 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:15:10,512 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2022-03-01 15:15:10,518 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2022-03-01 15:15:10,518 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1908244 bytes\n",
      "2022-03-01 15:15:10,738 INFO reduce.MergeManagerImpl: Merged 1 segments, 1908254 bytes to disk to satisfy reduce memory limit\n",
      "2022-03-01 15:15:10,739 INFO reduce.MergeManagerImpl: Merging 1 files, 1908258 bytes from disk\n",
      "2022-03-01 15:15:10,739 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2022-03-01 15:15:10,739 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2022-03-01 15:15:10,740 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1908244 bytes\n",
      "2022-03-01 15:15:10,741 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:15:10,752 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
      "2022-03-01 15:15:10,757 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2022-03-01 15:15:10,758 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2022-03-01 15:15:10,787 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:10,788 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:10,789 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:10,795 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:10,828 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:10,875 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-03-01 15:15:10,931 INFO streaming.PipeMapRed: Records R/W=18368/1\n",
      "2022-03-01 15:15:11,249 INFO streaming.PipeMapRed: R/W/S=100000/11612/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:11,475 INFO streaming.PipeMapRed: R/W/S=200000/21522/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:11,526 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2022-03-01 15:15:11,527 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2022-03-01 15:15:11,528 INFO mapred.Task: Task:attempt_local1969085402_0001_r_000000_0 is done. And is in the process of committing\n",
      "2022-03-01 15:15:11,530 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:15:11,531 INFO mapred.Task: Task attempt_local1969085402_0001_r_000000_0 is allowed to commit now\n",
      "2022-03-01 15:15:11,533 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1969085402_0001_r_000000_0' to file:/hduser/word_count/output\n",
      "2022-03-01 15:15:11,536 INFO mapred.LocalJobRunner: Records R/W=18368/1 > reduce\n",
      "2022-03-01 15:15:11,538 INFO mapred.Task: Task 'attempt_local1969085402_0001_r_000000_0' done.\n",
      "2022-03-01 15:15:11,539 INFO mapred.Task: Final Counters for attempt_local1969085402_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4956727\n",
      "\t\tFILE: Number of bytes written=4683257\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=23005\n",
      "\t\tReduce shuffle bytes=1908258\n",
      "\t\tReduce input records=203048\n",
      "\t\tReduce output records=23005\n",
      "\t\tSpilled Records=203048\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=31\n",
      "\t\tTotal committed heap usage (bytes)=362807296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=246758\n",
      "2022-03-01 15:15:11,539 INFO mapred.LocalJobRunner: Finishing task: attempt_local1969085402_0001_r_000000_0\n",
      "2022-03-01 15:15:11,540 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2022-03-01 15:15:11,876 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-03-01 15:15:11,877 INFO mapreduce.Job: Job job_local1969085402_0001 completed successfully\n",
      "2022-03-01 15:15:11,891 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6096906\n",
      "\t\tFILE: Number of bytes written=7211498\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=23305\n",
      "\t\tMap output records=203048\n",
      "\t\tMap output bytes=1502156\n",
      "\t\tMap output materialized bytes=1908258\n",
      "\t\tInput split bytes=90\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=23005\n",
      "\t\tReduce shuffle bytes=1908258\n",
      "\t\tReduce input records=203048\n",
      "\t\tReduce output records=23005\n",
      "\t\tSpilled Records=406096\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=31\n",
      "\t\tTotal committed heap usage (bytes)=725614592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1135453\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=246758\n",
      "2022-03-01 15:15:11,891 INFO streaming.StreamJob: Output directory: /hduser/word_count/output\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
    "-mapper 'python mapper.py unigram' -reducer 'python reducer.py' \\\n",
    "-input /hduser/word_count/input \\\n",
    "-output /hduser/word_count/output \\\n",
    "-file /content/map-reduce/mapper.py -file /content/map-reduce/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJZKfJKgJ_-9",
    "outputId": "8661b0f2-884d-4118-b7d1-fdd014e10097",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"'About\t1\n",
      "\"'After\t1\n",
      "\"'All\t1\n",
      "\"'An\t1\n",
      "\"'And\t1\n",
      "\"'Arthur\t1\n",
      "\"'At\t2\n",
      "\"'Black\t1\n",
      "\"'But\t4\n",
      "\"'Cause\t1\n",
      "\"'Consider,\t1\n",
      "\"'Does\t1\n",
      "\"'For\t1\n",
      "\"'Friends,'\t1\n",
      "\"'God\t1\n",
      "\"'Half\t1\n",
      "\"'He\t1\n",
      "\"'Here\t1\n",
      "\"'How\t1\n",
      "\"'Hum!'\t1\n",
      "\"'I\t11\n",
      "\"'IVY\t1\n",
      "\"'If\t1\n",
      "\"'It\t6\n",
      "\"'It's\t3\n",
      "\"'Jack\t1\n",
      "\"'Listen\t1\n",
      "\"'Look\t1\n",
      "\"'Mr.\t1\n",
      "\"'My\t1\n",
      "\"'No\t2\n",
      "\"'No;\t2\n",
      "\"'None\t1\n",
      "\"'Nonsense!'\t1\n",
      "\"'Nonsense,\t1\n",
      "\"'Not\t2\n",
      "\"'Nothing\t1\n",
      "\"'On\t1\n",
      "\"'Perhaps,\t1\n",
      "\"'Populus\t1\n",
      "\"'Possibly\t1\n",
      "\"'Quite\t1\n",
      "\"'Rache,'\t1\n",
      "\"'See\t1\n",
      "\"'So\t1\n",
      "\"'Take\t1\n",
      "\"'Tention!\"\t1\n",
      "\"'The\t1\n",
      "\"'Then\t1\n",
      "\"'There\t3\n",
      "\"'This\t2\n",
      "\"'Tis\t2\n",
      "\"'To\t1\n",
      "\"'We'll\t1\n",
      "\"'Well,\t3\n",
      "\"'Well?'\t1\n",
      "\"'What\t3\n",
      "\"'When\t1\n",
      "\"'Where\t1\n",
      "\"'Who\t1\n",
      "\"'Why,\t1\n",
      "\"'Would\t1\n",
      "\"'Yes.'\t3\n",
      "\"'You\t6\n",
      "\"'Your\t2\n",
      "\"--Holmes's\t1\n",
      "\"--diverting\t1\n",
      "\"--in\t1\n",
      "\"13,\t1\n",
      "\"1742.\"\t1\n",
      "\"1884.\"\t1\n",
      "\"A\t57\n",
      "\"About\t6\n",
      "\"Absolutely\t1\n",
      "\"After\t2\n",
      "\"Ah!\t6\n",
      "\"Ah,\t26\n",
      "\"All\t12\n",
      "\"Always!\"\t1\n",
      "\"Am\t1\n",
      "\"Amen!\t1\n",
      "\"American\t1\n",
      "\"American,\t1\n",
      "\"Ames,\t1\n",
      "\"Ames,\"\t1\n",
      "\"Among\t1\n",
      "\"An\t6\n",
      "\"And\t124\n",
      "\"And,\t1\n",
      "\"Any\t4\n",
      "\"Anyone\t1\n",
      "\"Anything\t4\n",
      "\"Apart\t1\n",
      "\"Are\t10\n",
      "\"Aren't\t1\n",
      "\"Arthur\t1\n",
      "\"As\t12\n",
      "\"Ask\t1\n",
      "\"At\t10\n",
      "\"Au\t1\n",
      "\"Away,\t1\n",
      "\"Ay,\t7\n",
      "\"Ay,\"\t1\n",
      "\"Baldwin--he\t1\n",
      "\"Barrymore\t2\n",
      "\"Bartholomew\t2\n",
      "\"Baskerville\t2\n",
      "\"Be\t2\n",
      "\"Bear\t"
     ]
    }
   ],
   "source": [
    "!hadoop fs -head /hduser/word_count/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CtSOXvEK57O"
   },
   "source": [
    "- Modify mapper.py as follows:\n",
    "  - Transform all characters in word to lowercase\n",
    "  - Remove special character\n",
    "- Run MapReduce again. Show your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ex8tvEMKQSJ",
    "outputId": "81329ad1-61ae-4651-e619-d4350b793471",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 15:15:34,156 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/content/map-reduce/mapper.py, /content/map-reduce/reducer.py] [] /tmp/streamjob3719629990838623494.jar tmpDir=null\n",
      "2022-03-01 15:15:34,889 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-03-01 15:15:35,005 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-03-01 15:15:35,005 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2022-03-01 15:15:35,027 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2022-03-01 15:15:35,171 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-03-01 15:15:35,189 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2022-03-01 15:15:35,504 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local347159830_0001\n",
      "2022-03-01 15:15:35,505 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-03-01 15:15:35,896 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local347159830_0001_88edda9e-c591-4a0c-80d0-ca74aa182d3a/mapper.py\n",
      "2022-03-01 15:15:35,936 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local347159830_0001_b79413ea-956d-46d8-88d6-74a857c16fa5/reducer.py\n",
      "2022-03-01 15:15:36,062 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2022-03-01 15:15:36,064 INFO mapreduce.Job: Running job: job_local347159830_0001\n",
      "2022-03-01 15:15:36,070 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2022-03-01 15:15:36,072 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2022-03-01 15:15:36,079 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:15:36,079 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:15:36,135 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2022-03-01 15:15:36,139 INFO mapred.LocalJobRunner: Starting task: attempt_local347159830_0001_m_000000_0\n",
      "2022-03-01 15:15:36,169 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:15:36,172 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:15:36,212 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2022-03-01 15:15:36,226 INFO mapred.MapTask: Processing split: file:/hduser/word_count/input/text.txt:0+1126637\n",
      "2022-03-01 15:15:36,250 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2022-03-01 15:15:36,313 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2022-03-01 15:15:36,313 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2022-03-01 15:15:36,313 INFO mapred.MapTask: soft limit at 83886080\n",
      "2022-03-01 15:15:36,313 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2022-03-01 15:15:36,313 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2022-03-01 15:15:36,317 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2022-03-01 15:15:36,325 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py, unigram, t]\n",
      "2022-03-01 15:15:36,335 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2022-03-01 15:15:36,336 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2022-03-01 15:15:36,337 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2022-03-01 15:15:36,337 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2022-03-01 15:15:36,338 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2022-03-01 15:15:36,338 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2022-03-01 15:15:36,339 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2022-03-01 15:15:36,339 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2022-03-01 15:15:36,339 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2022-03-01 15:15:36,340 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2022-03-01 15:15:36,341 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2022-03-01 15:15:36,341 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2022-03-01 15:15:36,381 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:36,382 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:36,383 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:36,391 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:36,481 INFO streaming.PipeMapRed: Records R/W=2682/1\n",
      "2022-03-01 15:15:36,680 INFO streaming.PipeMapRed: R/W/S=10000/51754/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:37,006 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2022-03-01 15:15:37,007 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2022-03-01 15:15:37,010 INFO mapred.LocalJobRunner: \n",
      "2022-03-01 15:15:37,010 INFO mapred.MapTask: Starting flush of map output\n",
      "2022-03-01 15:15:37,010 INFO mapred.MapTask: Spilling map output\n",
      "2022-03-01 15:15:37,010 INFO mapred.MapTask: bufstart = 0; bufend = 1470686; bufvoid = 104857600\n",
      "2022-03-01 15:15:37,011 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25386876(101547504); length = 827521/6553600\n",
      "2022-03-01 15:15:37,069 INFO mapreduce.Job: Job job_local347159830_0001 running in uber mode : false\n",
      "2022-03-01 15:15:37,071 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-03-01 15:15:37,607 INFO mapred.MapTask: Finished spill 0\n",
      "2022-03-01 15:15:37,620 INFO mapred.Task: Task:attempt_local347159830_0001_m_000000_0 is done. And is in the process of committing\n",
      "2022-03-01 15:15:37,622 INFO mapred.LocalJobRunner: Records R/W=2682/1\n",
      "2022-03-01 15:15:37,622 INFO mapred.Task: Task 'attempt_local347159830_0001_m_000000_0' done.\n",
      "2022-03-01 15:15:37,630 INFO mapred.Task: Final Counters for attempt_local347159830_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1140179\n",
      "\t\tFILE: Number of bytes written=2501461\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=23305\n",
      "\t\tMap output records=206881\n",
      "\t\tMap output bytes=1470686\n",
      "\t\tMap output materialized bytes=1884454\n",
      "\t\tInput split bytes=90\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=206881\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=387973120\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1135453\n",
      "2022-03-01 15:15:37,630 INFO mapred.LocalJobRunner: Finishing task: attempt_local347159830_0001_m_000000_0\n",
      "2022-03-01 15:15:37,630 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2022-03-01 15:15:37,635 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2022-03-01 15:15:37,638 INFO mapred.LocalJobRunner: Starting task: attempt_local347159830_0001_r_000000_0\n",
      "2022-03-01 15:15:37,647 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:15:37,647 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:15:37,647 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2022-03-01 15:15:37,652 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@43d366fb\n",
      "2022-03-01 15:15:37,654 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2022-03-01 15:15:37,677 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2022-03-01 15:15:37,694 INFO reduce.EventFetcher: attempt_local347159830_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2022-03-01 15:15:37,733 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local347159830_0001_m_000000_0 decomp: 1884450 len: 1884454 to MEMORY\n",
      "2022-03-01 15:15:37,737 INFO reduce.InMemoryMapOutput: Read 1884450 bytes from map-output for attempt_local347159830_0001_m_000000_0\n",
      "2022-03-01 15:15:37,742 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1884450, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1884450\n",
      "2022-03-01 15:15:37,746 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2022-03-01 15:15:37,747 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:15:37,748 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2022-03-01 15:15:37,754 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2022-03-01 15:15:37,754 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1884446 bytes\n",
      "2022-03-01 15:15:37,992 INFO reduce.MergeManagerImpl: Merged 1 segments, 1884450 bytes to disk to satisfy reduce memory limit\n",
      "2022-03-01 15:15:37,993 INFO reduce.MergeManagerImpl: Merging 1 files, 1884454 bytes from disk\n",
      "2022-03-01 15:15:37,994 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2022-03-01 15:15:37,994 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2022-03-01 15:15:37,995 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1884446 bytes\n",
      "2022-03-01 15:15:37,996 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:15:38,017 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
      "2022-03-01 15:15:38,024 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2022-03-01 15:15:38,026 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2022-03-01 15:15:38,060 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,061 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,066 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,074 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-03-01 15:15:38,085 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,113 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,283 INFO streaming.PipeMapRed: Records R/W=36941/1\n",
      "2022-03-01 15:15:38,510 INFO streaming.PipeMapRed: R/W/S=100000/4746/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,804 INFO streaming.PipeMapRed: R/W/S=200000/10322/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:15:38,872 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2022-03-01 15:15:38,873 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2022-03-01 15:15:38,874 INFO mapred.Task: Task:attempt_local347159830_0001_r_000000_0 is done. And is in the process of committing\n",
      "2022-03-01 15:15:38,876 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:15:38,876 INFO mapred.Task: Task attempt_local347159830_0001_r_000000_0 is allowed to commit now\n",
      "2022-03-01 15:15:38,878 INFO output.FileOutputCommitter: Saved output of task 'attempt_local347159830_0001_r_000000_0' to file:/hduser/word_count/output_q3\n",
      "2022-03-01 15:15:38,879 INFO mapred.LocalJobRunner: Records R/W=36941/1 > reduce\n",
      "2022-03-01 15:15:38,879 INFO mapred.Task: Task 'attempt_local347159830_0001_r_000000_0' done.\n",
      "2022-03-01 15:15:38,880 INFO mapred.Task: Final Counters for attempt_local347159830_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4909119\n",
      "\t\tFILE: Number of bytes written=4508618\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11830\n",
      "\t\tReduce shuffle bytes=1884454\n",
      "\t\tReduce input records=206881\n",
      "\t\tReduce output records=11830\n",
      "\t\tSpilled Records=206881\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=387973120\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=122703\n",
      "2022-03-01 15:15:38,880 INFO mapred.LocalJobRunner: Finishing task: attempt_local347159830_0001_r_000000_0\n",
      "2022-03-01 15:15:38,880 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2022-03-01 15:15:39,074 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-03-01 15:15:39,075 INFO mapreduce.Job: Job job_local347159830_0001 completed successfully\n",
      "2022-03-01 15:15:39,089 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6049298\n",
      "\t\tFILE: Number of bytes written=7010079\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=23305\n",
      "\t\tMap output records=206881\n",
      "\t\tMap output bytes=1470686\n",
      "\t\tMap output materialized bytes=1884454\n",
      "\t\tInput split bytes=90\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11830\n",
      "\t\tReduce shuffle bytes=1884454\n",
      "\t\tReduce input records=206881\n",
      "\t\tReduce output records=11830\n",
      "\t\tSpilled Records=413762\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=775946240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1135453\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=122703\n",
      "2022-03-01 15:15:39,089 INFO streaming.StreamJob: Output directory: /hduser/word_count/output_q3\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
    "-mapper 'python mapper.py unigram t' -reducer 'python reducer.py' \\\n",
    "-input /hduser/word_count/input \\\n",
    "-output /hduser/word_count/output_q3 \\\n",
    "-file /content/map-reduce/mapper.py -file /content/map-reduce/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39S2TSQBM0mO",
    "outputId": "a8e17f47-8055-491b-9cf1-91198febc1b4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t6\n",
      "10\t5\n",
      "109\t1\n",
      "11\t5\n",
      "12\t3\n",
      "127\t1\n",
      "129\t1\n",
      "13\t7\n",
      "13th\t1\n",
      "14\t3\n",
      "14th\t1\n",
      "15\t4\n",
      "1543\t1\n",
      "15th\t1\n",
      "16\t1\n",
      "1642\t1\n",
      "1644\t1\n",
      "1647\t1\n",
      "16th\t1\n",
      "17\t1\n",
      "171\t1\n",
      "1730\t1\n",
      "1742\t2\n",
      "1750\t2\n",
      "17th\t1\n",
      "1800\t1\n",
      "1857\t1\n",
      "1860\t1\n",
      "1865\t1\n",
      "1871\t1\n",
      "1872\t1\n",
      "1874\t1\n",
      "1875\t2\n",
      "1876\t1\n",
      "1878\t3\n",
      "1882\t6\n",
      "1883\t1\n",
      "1884\t2\n",
      "18th\t1\n",
      "19\t1\n",
      "2\t8\n",
      "20\t1\n",
      "200\t1\n",
      "21\t2\n",
      "22\t1\n",
      "221b\t4\n",
      "23\t1\n",
      "24\t2\n",
      "249\t1\n",
      "25\t1\n",
      "26\t2\n",
      "27\t2\n",
      "2704\t3\n",
      "28\t1\n",
      "28th\t1\n",
      "29\t4\n",
      "293\t1\n",
      "3\t14\n",
      "30\t2\n",
      "31\t1\n",
      "34\t1\n",
      "340\t1\n",
      "341\t9\n",
      "34th\t2\n",
      "36\t1\n",
      "37\t2\n",
      "3d\t3\n",
      "4\t7\n",
      "41\t1\n",
      "46\t2\n",
      "47\t1\n",
      "4th\t5\n",
      "5\t7\n",
      "534\t8\n",
      "6\t6\n",
      "66\t1\n",
      "6th\t1\n",
      "7\t8\n",
      "76\t1\n",
      "8\t5\n",
      "80\t1\n",
      "9\t6\n",
      "97163\t1\n",
      "a\t4737\n",
      "aback\t2\n",
      "abandon\t7\n",
      "abandoned\t7\n",
      "abandoning\t1\n",
      "abandons\t1\n",
      "abashed\t1\n",
      "abdullah\t8\n",
      "abelwhite\t4\n",
      "aberdeen\t1\n",
      "aberdonian\t2\n",
      "abetting\t1\n",
      "abhor\t2\n",
      "abiding\t2\n",
      "abilities\t1\n",
      "ability\t2\n",
      "able\t73\n",
      "aboard\t4\n",
      "abode\t5\n",
      "aborigines\t1\n",
      "abortive\t1\n",
      "about\t355\n",
      "above\t53\n",
      "abreast\t1\n",
      "abroad\t2\n",
      "abrupt\t1\n",
      "abruptly\t2\n",
      "absence\t13\n",
      "absent\t14\n",
      "absentee\t1\n",
      "absolute\t6\n",
      "absolutely\t9\n",
      "absorb\t1\n",
      "absorbed\t12\n",
      "absorbing\t4\n",
      "abstainers\t1\n",
      "abstract\t2\n",
      "abstracted\t3\n",
      "abstraction\t1\n",
      "abstruse\t2\n",
      "absurd\t5\n",
      "absurdity\t1\n",
      "absurdly\t1\n",
      "abused\t2\n",
      "abusing\t1\n",
      "abyss\t1\n",
      "ac\t1\n",
      "accent\t6\n",
      "accept\t9\n",
      "accepted\t4\n",
      "access\t2\n",
      "accessory\t2\n",
      "a"
     ]
    }
   ],
   "source": [
    "!hadoop fs -head /hduser/word_count/output_q3/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1Foy-4Ligi0"
   },
   "source": [
    "- How many times that the word `data` appears?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icUAbud6if5N",
    "outputId": "caaff72d-637a-4ffd-c221-7b6e4a532509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t8\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /hduser/word_count/output_q3/part-00000 | grep \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7JItps-EB8P"
   },
   "source": [
    "> **Observation**\n",
    "> - The word `data` appeared in the text for `8` times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x3JaYsBjAPW"
   },
   "source": [
    "- Modify mapper.py to capture the combination of 2 neighbouring words (Bigram)\n",
    "- Run MapReduce again. Show your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef51lWQNiWU2",
    "outputId": "0ca7b186-1599-4fcc-fc4b-cf5b3936a2e8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 15:16:03,075 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/content/map-reduce/mapper.py, /content/map-reduce/reducer.py] [] /tmp/streamjob1978822126841633554.jar tmpDir=null\n",
      "2022-03-01 15:16:03,721 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2022-03-01 15:16:03,863 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2022-03-01 15:16:03,864 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2022-03-01 15:16:03,886 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2022-03-01 15:16:04,084 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-03-01 15:16:04,106 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2022-03-01 15:16:04,409 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1618948256_0001\n",
      "2022-03-01 15:16:04,409 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-03-01 15:16:04,821 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1618948256_0001_028af927-51f8-46bd-b1e0-74cfdedc4a7f/mapper.py\n",
      "2022-03-01 15:16:04,848 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map-reduce/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1618948256_0001_dd6d27ad-5dc0-4741-9534-788f42ffdb51/reducer.py\n",
      "2022-03-01 15:16:04,942 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2022-03-01 15:16:04,944 INFO mapreduce.Job: Running job: job_local1618948256_0001\n",
      "2022-03-01 15:16:04,950 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2022-03-01 15:16:04,952 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2022-03-01 15:16:04,959 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:16:04,959 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:16:05,017 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2022-03-01 15:16:05,023 INFO mapred.LocalJobRunner: Starting task: attempt_local1618948256_0001_m_000000_0\n",
      "2022-03-01 15:16:05,062 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:16:05,065 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:16:05,100 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2022-03-01 15:16:05,115 INFO mapred.MapTask: Processing split: file:/hduser/word_count/input/text.txt:0+1126637\n",
      "2022-03-01 15:16:05,137 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2022-03-01 15:16:05,211 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2022-03-01 15:16:05,211 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2022-03-01 15:16:05,211 INFO mapred.MapTask: soft limit at 83886080\n",
      "2022-03-01 15:16:05,211 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2022-03-01 15:16:05,211 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2022-03-01 15:16:05,215 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2022-03-01 15:16:05,231 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py, bigram, t]\n",
      "2022-03-01 15:16:05,245 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2022-03-01 15:16:05,246 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2022-03-01 15:16:05,246 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2022-03-01 15:16:05,247 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2022-03-01 15:16:05,248 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2022-03-01 15:16:05,248 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2022-03-01 15:16:05,248 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2022-03-01 15:16:05,249 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2022-03-01 15:16:05,249 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2022-03-01 15:16:05,250 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2022-03-01 15:16:05,250 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2022-03-01 15:16:05,250 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2022-03-01 15:16:05,285 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:05,286 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:05,288 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:05,305 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:05,403 INFO streaming.PipeMapRed: Records R/W=2682/1\n",
      "2022-03-01 15:16:05,623 INFO streaming.PipeMapRed: R/W/S=10000/53092/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:05,915 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2022-03-01 15:16:05,916 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2022-03-01 15:16:05,920 INFO mapred.LocalJobRunner: \n",
      "2022-03-01 15:16:05,920 INFO mapred.MapTask: Starting flush of map output\n",
      "2022-03-01 15:16:05,920 INFO mapred.MapTask: Spilling map output\n",
      "2022-03-01 15:16:05,920 INFO mapred.MapTask: bufstart = 0; bufend = 2288496; bufvoid = 104857600\n",
      "2022-03-01 15:16:05,920 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25460408(101841632); length = 753989/6553600\n",
      "2022-03-01 15:16:05,949 INFO mapreduce.Job: Job job_local1618948256_0001 running in uber mode : false\n",
      "2022-03-01 15:16:05,950 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-03-01 15:16:06,940 INFO mapred.MapTask: Finished spill 0\n",
      "2022-03-01 15:16:06,954 INFO mapred.Task: Task:attempt_local1618948256_0001_m_000000_0 is done. And is in the process of committing\n",
      "2022-03-01 15:16:06,956 INFO mapred.LocalJobRunner: Records R/W=2682/1\n",
      "2022-03-01 15:16:06,956 INFO mapred.Task: Task 'attempt_local1618948256_0001_m_000000_0' done.\n",
      "2022-03-01 15:16:06,964 INFO mapred.Task: Final Counters for attempt_local1618948256_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1140179\n",
      "\t\tFILE: Number of bytes written=3285487\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=23305\n",
      "\t\tMap output records=188498\n",
      "\t\tMap output bytes=2288496\n",
      "\t\tMap output materialized bytes=2665498\n",
      "\t\tInput split bytes=90\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=188498\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=392167424\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1135453\n",
      "2022-03-01 15:16:06,964 INFO mapred.LocalJobRunner: Finishing task: attempt_local1618948256_0001_m_000000_0\n",
      "2022-03-01 15:16:06,964 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2022-03-01 15:16:06,971 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2022-03-01 15:16:06,971 INFO mapred.LocalJobRunner: Starting task: attempt_local1618948256_0001_r_000000_0\n",
      "2022-03-01 15:16:06,981 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2022-03-01 15:16:06,981 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2022-03-01 15:16:06,982 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2022-03-01 15:16:06,985 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@293f7d05\n",
      "2022-03-01 15:16:06,986 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2022-03-01 15:16:07,009 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2022-03-01 15:16:07,024 INFO reduce.EventFetcher: attempt_local1618948256_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2022-03-01 15:16:07,060 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1618948256_0001_m_000000_0 decomp: 2665494 len: 2665498 to MEMORY\n",
      "2022-03-01 15:16:07,067 INFO reduce.InMemoryMapOutput: Read 2665494 bytes from map-output for attempt_local1618948256_0001_m_000000_0\n",
      "2022-03-01 15:16:07,069 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2665494, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2665494\n",
      "2022-03-01 15:16:07,074 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2022-03-01 15:16:07,075 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:16:07,075 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2022-03-01 15:16:07,082 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2022-03-01 15:16:07,082 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2665480 bytes\n",
      "2022-03-01 15:16:07,289 INFO reduce.MergeManagerImpl: Merged 1 segments, 2665494 bytes to disk to satisfy reduce memory limit\n",
      "2022-03-01 15:16:07,290 INFO reduce.MergeManagerImpl: Merging 1 files, 2665498 bytes from disk\n",
      "2022-03-01 15:16:07,291 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2022-03-01 15:16:07,291 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2022-03-01 15:16:07,292 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2665480 bytes\n",
      "2022-03-01 15:16:07,293 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:16:07,322 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
      "2022-03-01 15:16:07,333 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2022-03-01 15:16:07,335 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2022-03-01 15:16:07,360 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:07,360 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:07,362 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:07,376 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:07,419 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:07,487 INFO streaming.PipeMapRed: Records R/W=11305/1\n",
      "2022-03-01 15:16:07,944 INFO streaming.PipeMapRed: R/W/S=100000/40463/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2022-03-01 15:16:07,953 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-03-01 15:16:08,310 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2022-03-01 15:16:08,310 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2022-03-01 15:16:08,311 INFO mapred.Task: Task:attempt_local1618948256_0001_r_000000_0 is done. And is in the process of committing\n",
      "2022-03-01 15:16:08,313 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2022-03-01 15:16:08,313 INFO mapred.Task: Task attempt_local1618948256_0001_r_000000_0 is allowed to commit now\n",
      "2022-03-01 15:16:08,315 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1618948256_0001_r_000000_0' to file:/hduser/word_count/output_q7\n",
      "2022-03-01 15:16:08,319 INFO mapred.LocalJobRunner: Records R/W=11305/1 > reduce\n",
      "2022-03-01 15:16:08,319 INFO mapred.Task: Task 'attempt_local1618948256_0001_r_000000_0' done.\n",
      "2022-03-01 15:16:08,320 INFO mapred.Task: Final Counters for attempt_local1618948256_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6471207\n",
      "\t\tFILE: Number of bytes written=7077886\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=81254\n",
      "\t\tReduce shuffle bytes=2665498\n",
      "\t\tReduce input records=188498\n",
      "\t\tReduce output records=81254\n",
      "\t\tSpilled Records=188498\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=15\n",
      "\t\tTotal committed heap usage (bytes)=392167424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1126901\n",
      "2022-03-01 15:16:08,320 INFO mapred.LocalJobRunner: Finishing task: attempt_local1618948256_0001_r_000000_0\n",
      "2022-03-01 15:16:08,320 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2022-03-01 15:16:08,953 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-03-01 15:16:08,954 INFO mapreduce.Job: Job job_local1618948256_0001 completed successfully\n",
      "2022-03-01 15:16:08,964 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7611386\n",
      "\t\tFILE: Number of bytes written=10363373\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=23305\n",
      "\t\tMap output records=188498\n",
      "\t\tMap output bytes=2288496\n",
      "\t\tMap output materialized bytes=2665498\n",
      "\t\tInput split bytes=90\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=81254\n",
      "\t\tReduce shuffle bytes=2665498\n",
      "\t\tReduce input records=188498\n",
      "\t\tReduce output records=81254\n",
      "\t\tSpilled Records=376996\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=15\n",
      "\t\tTotal committed heap usage (bytes)=784334848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1135453\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1126901\n",
      "2022-03-01 15:16:08,965 INFO streaming.StreamJob: Output directory: /hduser/word_count/output_q7\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
    "-mapper 'python mapper.py bigram t' -reducer 'python reducer.py' \\\n",
    "-input /hduser/word_count/input \\\n",
    "-output /hduser/word_count/output_q7 \\\n",
    "-file /content/map-reduce/mapper.py -file /content/map-reduce/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "youFxPfhkfc-",
    "outputId": "58fc7b33-acc2-4b8e-f230-6c996f095a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-knowledge\t1\n",
      "1-mr\t1\n",
      "1-the\t3\n",
      "10-30\t1\n",
      "10-and\t1\n",
      "10-extract\t1\n",
      "10-plays\t1\n",
      "109-293\t1\n",
      "11-he\t1\n",
      "11-in\t1\n",
      "11-is\t1\n",
      "11-the\t1\n",
      "12-death\t1\n",
      "12-has\t1\n",
      "127-36\t1\n",
      "129-camberwell\t1\n",
      "13-127\t1\n",
      "13-duncan\t2\n",
      "13-fixing\t1\n",
      "13-he\t1\n",
      "14-the\t1\n",
      "14th-of\t1\n",
      "15-a\t1\n",
      "15-and\t1\n",
      "15-why\t1\n",
      "1543-and\t1\n",
      "16-cushion\t1\n",
      "1642-charles\t1\n",
      "1644-of\t1\n",
      "1647-are\t1\n",
      "16th-a\t1\n",
      "17-21\t1\n",
      "1742-dr\t1\n",
      "1750-and\t1\n",
      "1750-or\t1\n",
      "17th-all\t1\n",
      "1800-i\t1\n",
      "1857-and\t1\n",
      "1865-a\t1\n",
      "1871-which\t1\n",
      "1875-it\t1\n",
      "1876-of\t1\n",
      "1878-i\t1\n",
      "1878-my\t1\n",
      "1878-nearly\t1\n",
      "1882-an\t1\n",
      "1882-do\t1\n",
      "1882-grimpen\t1\n",
      "1882-my\t1\n",
      "1882-to\t1\n",
      "1883-medical\t1\n",
      "1884-at\t1\n",
      "1884-it\t1\n",
      "18th-of\t1\n",
      "19-the\t1\n",
      "2-philosophy\t1\n",
      "2-sherlock\t1\n",
      "2-the\t3\n",
      "2-upon\t1\n",
      "200-pounds\t1\n",
      "21-41\t1\n",
      "21-cold\t1\n",
      "221b-baker\t2\n",
      "23-he\t1\n",
      "24-1872\t1\n",
      "25-and\t1\n",
      "26-fancy\t1\n",
      "27-as\t1\n",
      "27-had\t1\n",
      "2704-and\t1\n",
      "2704-is\t1\n",
      "2704-said\t1\n",
      "28-to\t1\n",
      "28th-of\t1\n",
      "29-but\t1\n",
      "29-chicago\t3\n",
      "293-5\t1\n",
      "3-37\t1\n",
      "3-astronomy\t1\n",
      "3-before\t1\n",
      "3-i\t1\n",
      "3-lauriston\t3\n",
      "3-lodge\t1\n",
      "3-mayfield\t1\n",
      "3-pinchin\t1\n",
      "3-the\t2\n",
      "3-turpey\t1\n",
      "30-and\t1\n",
      "30-train\t1\n",
      "31-4\t1\n",
      "34-do\t1\n",
      "340-miles\t1\n",
      "341-i\t1\n",
      "341-it\t1\n",
      "341-vermissa\t4\n",
      "341-were\t1\n",
      "341-whatever\t1\n",
      "34th-bombay\t1\n",
      "36-31\t1"
     ]
    }
   ],
   "source": [
    "!hadoop fs -head /hduser/word_count/output_q7/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLXBol3bkSrO"
   },
   "source": [
    "- How many times that `sherlock` and `holmes` appears together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yo54Ecotj2Oc",
    "outputId": "8ab21900-ec58-4fd9-ddd4-0595d81a42e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sherlock-holmes\t115\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /hduser/word_count/output_q7/part-00000 | grep \"sherlock-holmes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "D-5D0NB8EwlW"
   },
   "outputs": [],
   "source": [
    "!hadoop fs -cat /hduser/word_count/output_q7/part-00000 | grep \"holmes-sherlock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo4fGHpjEeNM"
   },
   "source": [
    "> **Observation**\n",
    "> - The words `sherlock`and `holmes` appeared together in the text for `115` times"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hadoop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
